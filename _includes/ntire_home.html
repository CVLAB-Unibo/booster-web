<section class="hero" id="paper">
    <div class="hero-body">
        <div class="container">
          <br>
          <h1 class="title has-text-centered">NTIRE 2023: HR Depth from Images of Specular and Transparent Surfaces</h1>
          <br>  
          We are delighted to inform you that the Booster dataset will be employed in the HR Depth from Images of Specular and Transparent Surfaces Challenge as a part of the <a href="https://cvlai.net/ntire/2023/">NTIRE workshop</a> in conjunction with CVPR 2023!
          <br>
          <br>
          
          <h1 class="subtitle">NEWS AND UPDATES</h1>
          <ul>
            <li>2023-03-04: Extended Workshop and Challenge Deadlines!</li>
            <li>2023-03-04: New Sponsorship! Prizes for each competiton winner! A big thank to <a href="https://www.eyecan.ai/">Eyecan</a>!</li>
            <li>2023-02-07: Training and validation data has been released. Codalab servers online.</li>
            <li>2022-12-16: Workshop proposal has been accepted.</li>
          </ul>  
          <br>
          <br>
  
          <h1 class="subtitle">SPONSORS</h1>
          <a href="https://www.eyecan.ai/"><img style="display: block;margin-left: auto;margin-right: auto;width:20%" src="assets/logo_eyecan.png"></a>
          <br>
          <a href="https://www.eyecan.ai/">Eyecan</a>, an emerging AutoAi startup, focusing on deep learning for robotics without the need for either manual annotations or synthetic data, sponsored our challenges, granting a prize of 500$ to the competition winner (if we reach a minimum number of valid submissions).
          <br>
          <br>

          <h1 class="subtitle">INTRODUCTION</h1>
          Depth estimation has a long history in computer vision and has been intensively studied for decades.
          <br>
          Deep learning succeeds in this field as well, with modern deep networks achieving ludricous error rates on established datasets 
          such as KITTI, Middlebury, etc.
          <br>
          <i>Should this evidence suggest that, thanks to deep learning, depth estimation is a solved problem?</i>
          <br>
          Definitely, no! It is time for the community to focus on the open-challenges left unsolved in the field. 
          In particular, the Booster dataset identifies two of such hazards:
          <ul>
            <li><b>Non-Lambertian surfaces, such as those of transparent/reflectant objects</b></li>
            <li><b>High-resolution images</b></li>
          </ul> 
          This challenge aims at fostering the community towards developing next-generation monocular or stereo depth networks capable of reasoning at an higher level, and thus yield accurate, 
          high-resolution 3D reconstructions for challenging objects, yet of common use.
          <br>
          <br>
  
          <h1 class="subtitle">CHALLENGE DESCRIPTION</h1>
          This challenge aims at estimating high-resolution disparity or depth maps from stereo or monocular images respectively.
          <br>
          The challenge will be divided into two phases:
          <ol>
            <li>
                <b>Model Construction:</b> 
                During this period, the partecipants will have to construct a model for the selected track (Monocular or Stereo). 
                The model can be trained using the Booster training split and any additional data. 
                The approach can be evaluated on the official validation set of each track.
            </li>
            <li>    
                <b>Testing Period:</b>
                During this period, the participants can submite the predictions of their model on the official test set. 
                The disparity/depth maps will be evaluated by the organizers with the quantitative metrics.
            </li>
          </ol>
          <br>
          <br>
          
          <h1 class="subtitle">TRACKS</h1>
          There will be two tracks for the challenge, both hosted by Codalab servers:
          <ul>
            <li><a href="https://codalab.lisn.upsaclay.fr/competitions/10494"><b>Track 1 - Stereo</b></a></li>
            <li><a href="https://codalab.lisn.upsaclay.fr/competitions/10502"><b>Track 2 - Mono</b></a></li>
          </ul>
          <br>
          <br>
  
  
          <h1 class="subtitle">DATASETS AND SUBMISSION</h1>
          All data can be downloaded from CodaLab.
          We use CodaLab servers for online submission in the development and test phase, testing the results on the validation set and test set respectively.
          After the test phase, the final results and the source codes (both training and test) need to be submitted via emails (boosterbenchmark@gmail.com). 
          Please refer to our online Codalab website for details of the submission rules [<a href="https://codalab.lisn.upsaclay.fr/competitions/10494">Codalab Stereo Track</a>], [<a href="https://codalab.lisn.upsaclay.fr/competitions/10502">Codalab Mono Track</a>].
          <br>
          <br>
  
          <h1 class="subtitle">IMPORTANT DATES</h1>
          <ul>
            <li>2023-02-0: Release of training and validation data;</li>
            <li>2023-02-07: Validation server online;</li>
            <li><s>2023-03-07</s> Extended 2023-03-14: Final test data release, validation server closed;</li>
            <li><s>2023-03-14</s> Extended 2023-03-20: Test result submission deadline;</li>
            <li><s>2023-03-15</s> Extended 2023-03-21: Fact sheet / code / model submission deadline;</li>
            <li><s>2023-03-17</s> Extended 2023-03-22: Test preliminary score release to the participants;</li>
            <li><s>2023-03-28</s> Extended 2023-03-31: Challenge paper submission deadline;</li>
          </ul>
          <br>
          <br>
      </div>
    </div>
</section>